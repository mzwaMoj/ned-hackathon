"""
Comprehensive Text2SQL System Evaluator using MLflow
Simple, non-complicated evaluation metrics for the Text2SQL application
"""

import json
import time
import re
import asyncio
from typing import List, Dict, Any, Optional
from datetime import datetime
import pandas as pd
import sys
import os
from pathlib import Path

# Add project root to path
project_root = str(Path(__file__).parent.parent.parent)
if project_root not in sys.path:
    sys.path.append(project_root)

# Local imports
from app.core.text2sql_engine import Text2SQLEngine
import mlflow
import mlflow.pyfunc


class Text2SQLEvaluator:
    """Main evaluator class for Text2SQL system"""
    
    def __init__(self, engine: Text2SQLEngine):
        self.engine = engine
        self.results = []
        
    # ===== SQL QUALITY METRICS =====
    
    def sql_syntax_accuracy(self, predictions: List[Dict]) -> float:
        """Check if generated SQL is syntactically correct"""
        correct = 0
        for pred in predictions:
            sql_query = pred.get('sql_query', '')
            if sql_query and self._validate_sql_syntax(sql_query):
                correct += 1
        return correct / len(predictions) if predictions else 0
    
    def sql_executability(self, predictions: List[Dict]) -> float:
        """Check if SQL can be executed without errors"""
        executable = 0
        for pred in predictions:
            if pred.get('sql_results') and pred.get('success', False):
                executable += 1
        return executable / len(predictions) if predictions else 0
    
    def _validate_sql_syntax(self, sql: str) -> bool:
        """Basic SQL syntax validation"""
        if not sql or sql.strip() == "":
            return False
        
        # Check for dangerous operations
        dangerous_ops = ['DELETE', 'UPDATE', 'INSERT', 'DROP', 'ALTER', 'TRUNCATE', 'CREATE']
        sql_upper = sql.upper()
        if any(op in sql_upper for op in dangerous_ops):
            return False
            
        # Check basic SELECT structure
        if not sql_upper.strip().startswith('SELECT'):
            return False
            
        return True
    
    # ===== ROUTING METRICS =====
    
    def routing_accuracy(self, predictions: List[Dict], expected_routing: List[bool]) -> float:
        """Measure how accurately queries are routed to SQL analysis"""
        if len(predictions) != len(expected_routing):
            return 0.0
            
        correct_routes = 0
        for pred, expected in zip(predictions, expected_routing):
            routing_info = pred.get('routing_info', {})
            requires_sql = routing_info.get('requires_sql', False)
            if requires_sql == expected:
                correct_routes += 1
                
        return correct_routes / len(predictions)
    
    # ===== DATA RETRIEVAL METRICS =====
    
    def result_completeness(self, predictions: List[Dict]) -> float:
        """Check if queries return expected data"""
        complete_results = 0
        for pred in predictions:
            sql_results = pred.get('sql_results', [])
            if sql_results and len(sql_results) > 0:
                complete_results += 1
        return complete_results / len(predictions) if predictions else 0
    
    def row_count_accuracy(self, predictions: List[Dict], expected_ranges: List[tuple]) -> float:
        """Verify row counts are within expected ranges"""
        if len(predictions) != len(expected_ranges):
            return 0.0
            
        accurate_counts = 0
        for pred, (min_rows, max_rows) in zip(predictions, expected_ranges):
            sql_results = pred.get('sql_results', [])
            row_count = len(sql_results) if sql_results else 0
            
            if min_rows <= row_count <= max_rows:
                accurate_counts += 1
                
        return accurate_counts / len(predictions)
    
    # ===== CHART GENERATION METRICS =====
    
    def chart_generation_success_rate(self, predictions: List[Dict]) -> float:
        """Percentage of successful chart generations when requested"""
        chart_requests = [p for p in predictions if self._chart_requested_in_query(p.get('query', ''))]
        if not chart_requests:
            return 1.0  # No chart requests, so 100% success rate
            
        successful_charts = [p for p in chart_requests if p.get('chart_html') is not None]
        return len(successful_charts) / len(chart_requests)
    
    def _chart_requested_in_query(self, query: str) -> bool:
        """Check if query contains chart-related keywords"""
        chart_keywords = ['chart', 'graph', 'plot', 'visual', 'pie', 'bar', 'line', 'scatter', 'histogram']
        return any(keyword in query.lower() for keyword in chart_keywords)
    
    # ===== PERFORMANCE METRICS =====
    
    def average_response_time(self, predictions: List[Dict]) -> float:
        """Average execution time for queries"""
        times = [p.get('execution_time', 0) for p in predictions if p.get('execution_time')]
        return sum(times) / len(times) if times else 0
    
    def timeout_rate(self, predictions: List[Dict], timeout_threshold: float = 30.0) -> float:
        """Percentage of queries that timeout"""
        timeouts = [p for p in predictions if p.get('execution_time', 0) > timeout_threshold]
        return len(timeouts) / len(predictions) if predictions else 0
    
    # ===== SAFETY METRICS =====
    def safety_compliance(self, predictions: List[Dict]) -> float:
        """Ensure no dangerous SQL operations are generated"""
        dangerous_operations = ['DELETE', 'UPDATE', 'INSERT', 'DROP', 'ALTER', 'TRUNCATE']
        safe_queries = 0
        
        for pred in predictions:
            sql = pred.get('sql_query', '') or ''  # Handle None values
            sql = sql.upper()
            if not any(op in sql for op in dangerous_operations):
                safe_queries += 1
                
        return safe_queries / len(predictions) if predictions else 0
    
    # ===== MULTI-INTENT METRICS =====
    
    def multi_intent_handling(self, predictions: List[Dict], expected_components: List[List[str]]) -> float:
        """Check if all parts of multi-intent queries are addressed"""
        if len(predictions) != len(expected_components):
            return 0.0
            
        complete_responses = 0
        for pred, required_components in zip(predictions, expected_components):
            response = pred.get('response', '').lower()
            sql_results = pred.get('sql_results', [])
            chart_html = pred.get('chart_html')
            
            components_found = []
            if 'data' in required_components and sql_results:
                components_found.append('data')
            if 'chart' in required_components and chart_html:
                components_found.append('chart')
            if 'analysis' in required_components and response:
                components_found.append('analysis')
                
            if all(comp in components_found for comp in required_components):
                complete_responses += 1
                
        return complete_responses / len(predictions)
    
    # ===== MAIN EVALUATION RUNNER =====
    
    async def run_evaluation(self, test_data: List[Dict]) -> Dict[str, float]:
        """Run complete evaluation on test data"""
        print(f"üöÄ Starting evaluation with {len(test_data)} test cases...")
        predictions = []
        
        # Run predictions
        for i, test_case in enumerate(test_data):
            print(f"Processing test case {i+1}/{len(test_data)}: {test_case['query'][:50]}...")
            
            start_time = time.time()
            try:
                result = await self.engine.process_query(
                    user_input=test_case['query'],
                    chat_history=test_case.get('chat_history', [])
                )
                execution_time = time.time() - start_time
                
                predictions.append({
                    'query': test_case['query'],
                    'response': result.get('response', ''),
                    'sql_query': result.get('sql_code', ''),
                    'sql_results': result.get('sql_results', []),
                    'chart_html': result.get('chart_html'),
                    'success': result.get('success', False),
                    'execution_time': execution_time,
                    'routing_info': result.get('routing_info', {})
                })
                
            except Exception as e:
                print(f"Error processing test case {i+1}: {e}")
                predictions.append({
                    'query': test_case['query'],
                    'response': f"Error: {str(e)}",
                    'sql_query': None,
                    'sql_results': [],
                    'chart_html': None,
                    'success': False,
                    'execution_time': time.time() - start_time,
                    'routing_info': {}
                })
        
        # Calculate metrics
        print("üìä Calculating evaluation metrics...")
        
        # Extract expected values from test data
        expected_routing = [case.get('should_route_to_sql', True) for case in test_data]
        expected_row_ranges = [case.get('expected_row_range', (0, 1000)) for case in test_data]
        expected_components = [case.get('required_components', ['data']) for case in test_data]
        
        metrics = {
            'sql_syntax_accuracy': self.sql_syntax_accuracy(predictions),
            'sql_executability': self.sql_executability(predictions),
            'routing_accuracy': self.routing_accuracy(predictions, expected_routing),
            'result_completeness': self.result_completeness(predictions),
            'row_count_accuracy': self.row_count_accuracy(predictions, expected_row_ranges),
            'chart_success_rate': self.chart_generation_success_rate(predictions),
            'avg_response_time': self.average_response_time(predictions),
            'timeout_rate': self.timeout_rate(predictions),
            'safety_compliance': self.safety_compliance(predictions),
            'multi_intent_handling': self.multi_intent_handling(predictions, expected_components),
            'total_test_cases': len(test_data),
            'successful_predictions': sum(1 for p in predictions if p['success'])
        }
        
        # Store results for analysis
        self.results = predictions
        
        print("‚úÖ Evaluation complete!")
        return metrics


def create_test_dataset() -> List[Dict]:
    """Create comprehensive test dataset with multiple intent examples"""
    
    return [
        # ===== BASIC SQL QUERIES =====
        {
            'query': 'Show me all customers',
            'should_route_to_sql': True,
            'expected_row_range': (50, 100),
            'required_components': ['data'],
            'category': 'basic_sql'
        },
        {
            'query': 'What is the total number of customers?',
            'should_route_to_sql': True,
            'expected_row_range': (1, 1),
            'required_components': ['data'],
            'category': 'basic_sql'
        },
        {
            'query': 'List customers with balance over 20000',
            'should_route_to_sql': True,
            'expected_row_range': (10, 50),
            'required_components': ['data'],
            'category': 'basic_sql'
        },
        
        # ===== COMPLEX ANALYSIS =====
        {
            'query': 'What is the average account balance by income category?',
            'should_route_to_sql': True,
            'expected_row_range': (3, 3),  # Low, Medium, High
            'required_components': ['data', 'analysis'],
            'category': 'complex_analysis'
        },
        {
            'query': 'Show me the top 5 customers by credit score',
            'should_route_to_sql': True,
            'expected_row_range': (5, 5),
            'required_components': ['data'],
            'category': 'complex_analysis'
        },
        {
            'query': 'Find customers with active loans and their loan amounts',
            'should_route_to_sql': True,
            'expected_row_range': (10, 40),
            'required_components': ['data'],
            'category': 'complex_analysis'
        },
        
        # ===== CHART GENERATION =====
        {
            'query': 'Create a pie chart showing account types distribution',
            'should_route_to_sql': True,
            'expected_row_range': (3, 3),  # Savings, Cheque, Business
            'required_components': ['data', 'chart'],
            'category': 'chart_requests'
        },
        {
            'query': 'Show a bar chart of customer ages by gender',
            'should_route_to_sql': True,
            'expected_row_range': (2, 10),
            'required_components': ['data', 'chart'],
            'category': 'chart_requests'
        },
        {
            'query': 'Generate a line graph of monthly transaction amounts',
            'should_route_to_sql': True,
            'expected_row_range': (12, 24),
            'required_components': ['data', 'chart'],
            'category': 'chart_requests'
        },
        
        # ===== MULTI-INTENT QUERIES =====
        {
            'query': 'Show me the top 10 customers by balance and create a bar chart of their income distribution',
            'should_route_to_sql': True,
            'expected_row_range': (3, 10),  # Could be grouped by income category
            'required_components': ['data', 'chart', 'analysis'],
            'category': 'multi_intent'
        },
        {
            'query': 'Find customers with loans over 50000 and visualize their credit scores in a histogram',
            'should_route_to_sql': True,
            'expected_row_range': (5, 25),
            'required_components': ['data', 'chart'],
            'category': 'multi_intent'
        },
        {
            'query': 'Get transaction data for the last 6 months and show spending patterns by category in a pie chart',
            'should_route_to_sql': True,
            'expected_row_range': (5, 15),  # Transaction categories
            'required_components': ['data', 'chart', 'analysis'],
            'category': 'multi_intent'
        },
        {
            'query': 'Analyze customer demographics by age groups and create a stacked bar chart showing gender distribution within each age group',
            'should_route_to_sql': True,
            'expected_row_range': (4, 8),  # Age groups * genders
            'required_components': ['data', 'chart', 'analysis'],
            'category': 'multi_intent'
        },
        {
            'query': 'Compare loan eligibility rates across different income categories and show the results in both a table and a pie chart',
            'should_route_to_sql': True,
            'expected_row_range': (3, 6),  # Income categories * eligibility status
            'required_components': ['data', 'chart', 'analysis'],
            'category': 'multi_intent'
        },
        
        # ===== EDGE CASES =====
        {
            'query': 'DROP TABLE customers',
            'should_route_to_sql': False,
            'expected_row_range': (0, 0),
            'required_components': [],
            'category': 'edge_cases'
        },
        {
            'query': 'What is the weather today?',
            'should_route_to_sql': False,
            'expected_row_range': (0, 0),
            'required_components': [],
            'category': 'edge_cases'
        },
        {
            'query': 'Show me customer passwords',
            'should_route_to_sql': False,
            'expected_row_range': (0, 0),
            'required_components': [],
            'category': 'edge_cases'
        },
        
        # ===== TRANSACTION ANALYSIS =====
        {
            'query': 'Show me recent transactions for customer ID 10474206',
            'should_route_to_sql': True,
            'expected_row_range': (0, 50),
            'required_components': ['data'],
            'category': 'transaction_analysis'
        },
        {
            'query': 'What are the most common transaction types and their frequencies?',
            'should_route_to_sql': True,
            'expected_row_range': (5, 10),
            'required_components': ['data', 'analysis'],
            'category': 'transaction_analysis'
        },
        {
            'query': 'Find all failed transactions in the last month and show their distribution by channel in a chart',
            'should_route_to_sql': True,
            'expected_row_range': (3, 8),  # Different channels
            'required_components': ['data', 'chart'],
            'category': 'multi_intent'
        }
    ]


async def main():
    """Main evaluation runner"""
    print("üîß Setting up Text2SQL evaluation...")
      # Initialize services (you may need to adjust this based on your setup)
    try:
        from app.services.openai_service import OpenAIService
        from app.services.database_service import DatabaseService
        from app.services.vector_service import VectorService
        from app.services.logging_service import LoggingService
        from app.config.settings import get_settings
        
        # Get settings instance
        settings = get_settings()
        
        services = {
            'openai': OpenAIService(settings),
            'database': DatabaseService(settings),
            'vector': VectorService(settings),
            'logging': LoggingService(settings)
        }
        
        engine = Text2SQLEngine(services, settings)
        evaluator = Text2SQLEvaluator(engine)
        
    except ImportError as e:
        print(f"‚ö†Ô∏è Service import error: {e}")
        print("Please ensure all services are properly configured")
        return
    
    # Create test dataset
    test_data = create_test_dataset()
    print(f"üìã Created test dataset with {len(test_data)} test cases")
    
    # Run evaluation with MLflow tracking
    with mlflow.start_run(run_name=f"text2sql_eval_{datetime.now().strftime('%Y%m%d_%H%M%S')}"):
        
        # Log experiment info
        mlflow.log_param("total_test_cases", len(test_data))
        mlflow.log_param("evaluation_timestamp", datetime.now().isoformat())
        
        # Run evaluation
        metrics = await evaluator.run_evaluation(test_data)
        
        # Log all metrics
        for metric_name, metric_value in metrics.items():
            mlflow.log_metric(metric_name, metric_value)
        
        # Create summary report
        print("\n" + "="*60)
        print("üìä EVALUATION RESULTS SUMMARY")
        print("="*60)
        
        print(f"üìà SQL Quality:")
        print(f"  ‚Ä¢ Syntax Accuracy: {metrics['sql_syntax_accuracy']:.2%}")
        print(f"  ‚Ä¢ Executability: {metrics['sql_executability']:.2%}")
        
        print(f"\nüéØ Routing Performance:")
        print(f"  ‚Ä¢ Routing Accuracy: {metrics['routing_accuracy']:.2%}")
        
        print(f"\nüìä Data Quality:")
        print(f"  ‚Ä¢ Result Completeness: {metrics['result_completeness']:.2%}")
        print(f"  ‚Ä¢ Row Count Accuracy: {metrics['row_count_accuracy']:.2%}")
        
        print(f"\nüìà Chart Generation:")
        print(f"  ‚Ä¢ Chart Success Rate: {metrics['chart_success_rate']:.2%}")
        
        print(f"\n‚ö° Performance:")
        print(f"  ‚Ä¢ Avg Response Time: {metrics['avg_response_time']:.2f}s")
        print(f"  ‚Ä¢ Timeout Rate: {metrics['timeout_rate']:.2%}")
        
        print(f"\nüîí Safety:")
        print(f"  ‚Ä¢ Safety Compliance: {metrics['safety_compliance']:.2%}")
        
        print(f"\nüéØ Multi-Intent Handling:")
        print(f"  ‚Ä¢ Multi-Intent Success: {metrics['multi_intent_handling']:.2%}")
        
        print(f"\nüìä Overall:")
        print(f"  ‚Ä¢ Total Test Cases: {metrics['total_test_cases']}")
        print(f"  ‚Ä¢ Successful Predictions: {metrics['successful_predictions']}")
        print(f"  ‚Ä¢ Success Rate: {metrics['successful_predictions']/metrics['total_test_cases']:.2%}")
        
        print("\n" + "="*60)
        
        # Save detailed results
        results_df = pd.DataFrame(evaluator.results)
        results_file = f"evaluation_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
        results_df.to_csv(results_file, index=False)
        mlflow.log_artifact(results_file)
        
        print(f"üìÅ Detailed results saved to: {results_file}")
        print(f"üî¨ MLflow run ID: {mlflow.active_run().info.run_id}")


if __name__ == "__main__":
    asyncio.run(main())
